{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "47bfb290",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def group_text_by_time_window(result: dict, duration: int, time_window_size: int = 30):\n",
    "    \"\"\"\n",
    "    Group text output from OpenAI whisper into time windows\n",
    "    :param result: dict, output from OpenAI whisper model\n",
    "    :param time_window_size: int, size of time window in seconds\n",
    "\n",
    "    :return: dict, start time in seconds as key, text as value\n",
    "    \"\"\"\n",
    "    # get transcript segments and their start times\n",
    "    seg_starts = [seg[\"start\"] for seg in result[\"segments\"]]\n",
    "    seg_text = [seg[\"text\"] for seg in result[\"segments\"]]\n",
    "\n",
    "    time_windows = {}\n",
    "\n",
    "    # group text into buckets\n",
    "    for time, text in zip(seg_starts, seg_text):\n",
    "        time_window = int(time // time_window_size)\n",
    "\n",
    "        if time_window not in time_windows:\n",
    "            time_windows[time_window] = [text]\n",
    "        else:\n",
    "            time_windows[time_window].append(text)\n",
    "    final_times = {}\n",
    "\n",
    "    # create lists of start and ends times\n",
    "    starts = [index * time_window_size for index in time_windows.keys()]\n",
    "    ends = starts[1:] + [duration]\n",
    "\n",
    "    final_result = []\n",
    "\n",
    "    # create list of texts with startime and end timex\n",
    "    for ((index, text), (start, end)) in zip(time_windows.items(), zip(starts, ends)):\n",
    "        final_result.append({\"start\": start, \"end\": end, \"text\": \"\".join(text)})\n",
    "\n",
    "    return final_result\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2265f414",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File downloaded successfully.\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "import os\n",
    "filename = \"audio.mp3\"\n",
    "\n",
    "if not os.path.exists(filename):\n",
    "    url = \"https://nyt.simplecastaudio.com/3026b665-46df-4d18-98e9-d1ce16bbb1df/episodes/dd430a54-e475-46bd-b9fb-97a359c4161c/audio/128/default.mp3/default.mp3_ywr3ahjkcgo_63f2a6a9bc78a0a3100fbc9a815bf42d_62565420.mp3?aid=rss_feed&amp;awCollectionId=3026b665-46df-4d18-98e9-d1ce16bbb1df&amp;awEpisodeId=dd430a54-e475-46bd-b9fb-97a359c4161c&amp;feed=82FI35Px&hash_redirect=1&x-total-bytes=62565420&x-ais-classified=unclassified&listeningSessionID=0CD_382_16__be48a1539bfd29c334b850a7a8cf37e16ec362de\"\n",
    "    \n",
    "    response = requests.get(url)\n",
    "    with open(filename, \"wb\") as file:\n",
    "        file.write(response.content)\n",
    "\n",
    "print(\"File downloaded successfully.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "806e8ed2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import os\n",
    "\n",
    "# Define the filename for pickling\n",
    "pickle_filename = \"transcription_obj.pickle\"\n",
    "\n",
    "# Check if the pickled file exists\n",
    "if os.path.exists(pickle_filename):\n",
    "    # Read the pickled file\n",
    "    with open(pickle_filename, \"rb\") as file:\n",
    "        transcription_obj = pickle.load(file)\n",
    "else:\n",
    "\n",
    "    import whisper\n",
    "    model = whisper.load_model(\"base\")\n",
    "    transcription_obj = model.transcribe(\"audio.mp3\", language='en', without_timestamps=True)\n",
    "    \n",
    "    with open(pickle_filename, \"wb\") as file:\n",
    "        pickle.dump(transcription_obj, file)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8a6f4b4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('transcript.txt', 'w') as file:\n",
    "    file.write(transcription_obj['text'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "42822593",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # import\n",
    "# from langchain.text_splitter import CharacterTextSplitter\n",
    "# from langchain_community.document_loaders import TextLoader\n",
    "# from langchain_community.embeddings.sentence_transformer import (\n",
    "#     SentenceTransformerEmbeddings,\n",
    "# )\n",
    "# from langchain_community.vectorstores import Chroma\n",
    "\n",
    "# # load the document and split it into chunks\n",
    "# loader = TextLoader(\"transcript.txt\")\n",
    "# documents = loader.load()\n",
    "\n",
    "\n",
    "# # split it into chunks\n",
    "# text_splitter = CharacterTextSplitter(chunk_size=1000, chunk_overlap=200)\n",
    "# docs = text_splitter.split_documents(documents)\n",
    "\n",
    "# # print(len(docs[0].page_content))\n",
    "# print(docs)\n",
    "\n",
    "\n",
    "\n",
    "# # # create the open-source embedding function\n",
    "# # embedding_function = SentenceTransformerEmbeddings(model_name=\"all-MiniLM-L6-v2\")\n",
    "\n",
    "# # # load it into Chroma\n",
    "# # db = Chroma.from_documents(docs, embedding_function)\n",
    "\n",
    "# # # query it\n",
    "# # query = \"What books were recommended in this podcast?\"\n",
    "# # docs = db.similarity_search(query)\n",
    "\n",
    "# # print results\n",
    "# # print(docs[0].page_content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "bff797ae",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.11/site-packages/langchain_core/_api/deprecation.py:117: LangChainDeprecationWarning: The class `langchain_community.embeddings.openai.OpenAIEmbeddings` was deprecated in langchain-community 0.0.9 and will be removed in 0.2.0. An updated version of the class exists in the langchain-openai package and should be used instead. To use it run `pip install -U langchain-openai` and import as `from langchain_openai import OpenAIEmbeddings`.\n",
      "  warn_deprecated(\n",
      "/usr/local/lib/python3.11/site-packages/langchain_core/_api/deprecation.py:117: LangChainDeprecationWarning: The class `langchain_community.llms.openai.OpenAI` was deprecated in langchain-community 0.0.10 and will be removed in 0.2.0. An updated version of the class exists in the langchain-openai package and should be used instead. To use it run `pip install -U langchain-openai` and import as `from langchain_openai import OpenAI`.\n",
      "  warn_deprecated(\n",
      "/usr/local/lib/python3.11/site-packages/langchain_core/_api/deprecation.py:117: LangChainDeprecationWarning: The function `run` was deprecated in LangChain 0.1.0 and will be removed in 0.2.0. Use invoke instead.\n",
      "  warn_deprecated(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " The recommended books were \"Operation Pedestal\" by Max Hastings, \"Into the Heart of Romans\" by N.T. Wright, and \"Manhunt: The 12-Day Chase for Lincoln's Killer\" by James Swanson.\n"
     ]
    }
   ],
   "source": [
    "from langchain.vectorstores import Chroma\n",
    "from langchain.embeddings import OpenAIEmbeddings\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain.llms import OpenAI\n",
    "from langchain.chains import RetrievalQA\n",
    "from langchain_core.vectorstores import VectorStoreRetriever\n",
    "from langchain.document_loaders import TextLoader\n",
    "from typing import List\n",
    "from langchain.schema import Document\n",
    "import os\n",
    "from langchain.output_parsers.openai_functions import JsonOutputFunctionsParser\n",
    "\n",
    "\n",
    "os.environ['OPENAI_API_KEY'] = \"\"\n",
    "\n",
    "class Genie:\n",
    "\n",
    "    def __init__(self, file_path: str):\n",
    "        self.file_path = file_path\n",
    "        self.loader = TextLoader(self.file_path)\n",
    "        self.documents = self.loader.load()\n",
    "        self.texts = self.text_split(self.documents)\n",
    "        self.vectordb = self.embeddings(self.texts)\n",
    "        retriever = VectorStoreRetriever(vectorstore=self.vectordb)\n",
    "        self.genie = RetrievalQA.from_llm(llm=OpenAI(), retriever=retriever)\n",
    "\n",
    "    @staticmethod\n",
    "    def text_split(documents: TextLoader):\n",
    "        text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=200)\n",
    "        texts = text_splitter.split_documents(documents)\n",
    "        return texts\n",
    "\n",
    "    @staticmethod\n",
    "    def embeddings(texts: List[Document]):\n",
    "        embeddings = OpenAIEmbeddings()\n",
    "        vectordb = Chroma.from_documents(texts, embeddings)\n",
    "        return vectordb\n",
    "\n",
    "    def ask(self, query: str):\n",
    "        return self.genie.run(query)\n",
    "\n",
    "\n",
    "genie = Genie(\"transcript.txt\")\n",
    "print(genie.ask(\"What books were recommended in this podcast?\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "66507c17",
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "cannot import name 'OpenAIEmbeddings' from 'langchain_community.chat_models' (/usr/local/lib/python3.11/site-packages/langchain_community/chat_models/__init__.py)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[11], line 8\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mlangchain_core\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mrunnables\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m RunnableLambda, RunnablePassthrough\n\u001b[1;32m      7\u001b[0m \u001b[38;5;66;03m# from langchain_openai import ChatOpenAI, OpenAIEmbeddings\u001b[39;00m\n\u001b[0;32m----> 8\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mlangchain_community\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mchat_models\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m ChatOpenAI, OpenAIEmbeddings\n",
      "\u001b[0;31mImportError\u001b[0m: cannot import name 'OpenAIEmbeddings' from 'langchain_community.chat_models' (/usr/local/lib/python3.11/site-packages/langchain_community/chat_models/__init__.py)"
     ]
    }
   ],
   "source": [
    "from operator import itemgetter\n",
    "\n",
    "from langchain_community.vectorstores import FAISS\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_core.runnables import RunnableLambda, RunnablePassthrough\n",
    "# from langchain_openai import ChatOpenAI, OpenAIEmbeddings\n",
    "from langchain_community.chat_models import ChatOpenAI, OpenAIEmbeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "61f6ae2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "file_path = \"transcript.txt\"\n",
    "loader = TextLoader(file_path)\n",
    "\n",
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=200)\n",
    "documents = loader.load()\n",
    "texts = text_splitter.split_documents(documents)\n",
    "\n",
    "vectorstore = FAISS.from_documents(texts, OpenAIEmbeddings())\n",
    "\n",
    "template = \"\"\"What books were recommended in this podcast?\"\"\"\n",
    "\n",
    "chat = ChatPromptTemplate.from_template(template)\n",
    "\n",
    "model = ChatOpenAI()\n",
    "\n",
    "chain = (\n",
    "    RunnablePassthrough()\n",
    "    | RunnableLambda(lambda x: chat.run(x))\n",
    "    | model\n",
    "    | StrOutputParser()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ba4decbc",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.11/site-packages/langchain_core/_api/deprecation.py:117: LangChainDeprecationWarning: The function `convert_pydantic_to_openai_function` was deprecated in LangChain 0.1.16 and will be removed in 0.2.0. Use langchain_core.utils.function_calling.convert_to_openai_function() instead.\n",
      "  warn_deprecated(\n",
      "/usr/local/lib/python3.11/site-packages/langchain_core/_api/deprecation.py:117: LangChainDeprecationWarning: The class `langchain_community.chat_models.openai.ChatOpenAI` was deprecated in langchain-community 0.0.10 and will be removed in 0.2.0. An updated version of the class exists in the langchain-openai package and should be used instead. To use it run `pip install -U langchain-openai` and import as `from langchain_openai import ChatOpenAI`.\n",
      "  warn_deprecated(\n"
     ]
    }
   ],
   "source": [
    "from langchain.output_parsers.openai_functions import JsonOutputFunctionsParser\n",
    "from langchain_community.chat_models import ChatOpenAI\n",
    "from langchain.chains.combine_documents import create_stuff_documents_chain\n",
    "from langchain.chains import create_retrieval_chain\n",
    "from langchain import hub\n",
    "from langchain_community.utils.openai_functions import (\n",
    "    convert_pydantic_to_openai_function,\n",
    ")\n",
    "from langchain.output_parsers.openai_functions import JsonKeyOutputFunctionsParser\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_core.pydantic_v1 import BaseModel, Field, validator\n",
    "from langchain.chains.openai_functions import create_structured_output_runnable\n",
    "\n",
    "\n",
    "class Book(BaseModel):\n",
    "    \"\"\"A book.\"\"\"\n",
    "    title: str = Field(description=\"Title of the book\")\n",
    "    author: str = Field(description=\"Author of the book\")\n",
    "\n",
    "class Books(BaseModel):\n",
    "    \"\"\"A list of books.\"\"\"\n",
    "    books: List[Book] = Field(description=\"A list of books\")\n",
    "\n",
    "\n",
    "openai_functions = [convert_pydantic_to_openai_function(Book)]\n",
    "\n",
    "\n",
    "def embeddings(texts: List[Document]):\n",
    "    embeddings = OpenAIEmbeddings()\n",
    "    vectordb = Chroma.from_documents(texts, embeddings)\n",
    "    return vectordb\n",
    "\n",
    "prompt = hub.pull(\"langchain-ai/retrieval-qa-chat\")\n",
    "model = ChatOpenAI()\n",
    "\n",
    "\n",
    "file_path = \"transcript.txt\"\n",
    "loader = TextLoader(file_path)\n",
    "\n",
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=200)\n",
    "documents = loader.load()\n",
    "texts = text_splitter.split_documents(documents)\n",
    "vectordb = embeddings(texts)\n",
    "\n",
    "retriever = VectorStoreRetriever(vectorstore=vectordb)\n",
    "# chain = create_stuff_documents_chain(\n",
    "#     model, prompt\n",
    "# )\n",
    "chain = create_structured_output_runnable(Books, model, prompt)\n",
    "chain = create_retrieval_chain(retriever, chain)\n",
    "# chain = create_retrieval_chain(retriever, combine_docs_chain)\n",
    "\n",
    "result = chain.invoke({\"input\": \"What books were recommended in this podcast?\"})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b567dd17",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input': 'What books were recommended in this podcast?',\n",
       " 'context': [Document(page_content=\"It's a Max Hastings book. He's one of my favorite military historians. It's called Operation pedestal and for you, military history buffs who listened to Ezra really phenomenal storytelling about a pivotal convoy to save Malta in 1942 when it was under siege, just incredible cast of characters and remarkable level of heroism. And it's a really tremendous book. The next one is a brand new book by N. T. Wright, who's a theologian and it's called Into the Heart of Romans. And this is for your theology buffs who are Ezra who listened to your show Ezra. And it really is making a really interesting argument that the book of Romans, this pivotal book and the New Testament has been in some important ways misinterpreted and that a more proper interpretation of Romans is one that actually has a more radical call to virtue. And then the next book is Back to Your History Buffs. And it's called Man Hunt, 12 day chase for Lincoln's killer. And it's by James Swanson. And if you pick it up, you start\", metadata={'source': 'transcript.txt'}),\n",
       "  Document(page_content=\"It's a Max Hastings book. He's one of my favorite military historians. It's called Operation pedestal and for you, military history buffs who listened to Ezra really phenomenal storytelling about a pivotal convoy to save Malta in 1942 when it was under siege, just incredible cast of characters and remarkable level of heroism. And it's a really tremendous book. The next one is a brand new book by N. T. Wright, who's a theologian and it's called Into the Heart of Romans. And this is for your theology buffs who are Ezra who listened to your show Ezra. And it really is making a really interesting argument that the book of Romans, this pivotal book and the New Testament has been in some important ways misinterpreted and that a more proper interpretation of Romans is one that actually has a more radical call to virtue. And then the next book is Back to Your History Buffs. And it's called Man Hunt, 12 day chase for Lincoln's killer. And it's by James Swanson. And if you pick it up, you start\", metadata={'source': 'transcript.txt'}),\n",
       "  Document(page_content=\"radical call to virtue. And then the next book is Back to Your History Buffs. And it's called Man Hunt, 12 day chase for Lincoln's killer. And it's by James Swanson. And if you pick it up, you start reading it and you realize it feels like I'm reading almost like a script for a television crime drama. It sort of takes the assassination of Lincoln and puts it into kind of a modern frame of how we think about solving cases. And how it's like a criminal procedural as well as a historical account. It's really good. It's very gripping read. And it made me ask a question given all that happened after he died was the killing of Abraham Lincoln, the most consequential political assassination in all of American history. And I think it might have been David French. Thank you very much. Thanks so much, Ezra. It's always a pleasure to join you. This episode of the Ezra Clancho is produced by Roland Hu, fact checking my Michel Harris with Kate Sinclair and Mary Marge Locker, our senior engineers\", metadata={'source': 'transcript.txt'}),\n",
       "  Document(page_content=\"radical call to virtue. And then the next book is Back to Your History Buffs. And it's called Man Hunt, 12 day chase for Lincoln's killer. And it's by James Swanson. And if you pick it up, you start reading it and you realize it feels like I'm reading almost like a script for a television crime drama. It sort of takes the assassination of Lincoln and puts it into kind of a modern frame of how we think about solving cases. And how it's like a criminal procedural as well as a historical account. It's really good. It's very gripping read. And it made me ask a question given all that happened after he died was the killing of Abraham Lincoln, the most consequential political assassination in all of American history. And I think it might have been David French. Thank you very much. Thanks so much, Ezra. It's always a pleasure to join you. This episode of the Ezra Clancho is produced by Roland Hu, fact checking my Michel Harris with Kate Sinclair and Mary Marge Locker, our senior engineers\", metadata={'source': 'transcript.txt'})],\n",
       " 'answer': Books(books=[Book(title='Operation Pedestal', author='Max Hastings'), Book(title='Into the Heart of Romans', author='N. T. Wright'), Book(title=\"Man Hunt: 12 Day Chase for Lincoln's Killer\", author='James Swanson')])}"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "96d6ddef",
   "metadata": {},
   "outputs": [],
   "source": [
    "result = result['answer']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "dd49546f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Operation Pedestal by Max Hastings\n",
      "Into the Heart of Romans by N. T. Wright\n",
      "Man Hunt: 12 Day Chase for Lincoln's Killer by James Swanson\n"
     ]
    }
   ],
   "source": [
    "for book in result.books:\n",
    "    print(f\"{book.title} by {book.author}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "d89df39b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'isbn10': '0063341085', 'isbn13': '978-0063341081', 'title': \"Operation Biting: The 1942 Parachute Assault to Capture Hitler's Radar\", 'author': 'Max Hastings', 'image_link': 'https://m.media-amazon.com/images/I/81HIJ+6qHWL._SL1500_.jpg'}\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import re\n",
    "\n",
    "# Define the base URL for Amazon searches\n",
    "search_url = \"https://www.amazon.com/s?k={}&i=stripbooks\"\n",
    "\n",
    "# Combine the book title and author into a search query\n",
    "book_title = \"Operation Pedestal\"\n",
    "author = \"Max Hastings\"\n",
    "query = f\"{book_title} {author}\"\n",
    "\n",
    "# add headers to make it look like a real browser\n",
    "headers = {\n",
    "    \"User-Agent\": \"Mozilla/5.0 (Macintosh; Intel Mac OS X 10.15; rv:124.0) Gecko/20100101 Firefox/124.0\",\n",
    "    \"Accept\": \"text/html,application/xhtml+xml,application/xml;q=0.9,image/avif,image/webp,*/*;q=0.8\",\n",
    "    \"Accept-Language\": \"en-US,en;q=0.5\",\n",
    "    \"Accept-Encoding\": \"gzip, deflate, br\",\n",
    "    \"DNT\": \"1\",\n",
    "    \"Connection\": \"keep-alive\",\n",
    "    \"Upgrade-Insecure-Requests\": \"1\",\n",
    "    \"Sec-Fetch-Dest\": \"document\",\n",
    "    \"Sec-Fetch-Mode\": \"navigate\",\n",
    "    \"Sec-Fetch-Site\": \"same-origin\",\n",
    "    \"Sec-GPC\": \"1\"\n",
    "}\n",
    "\n",
    "# Perform the search with headers\n",
    "response = requests.get(search_url.format(query.replace(\" \", \"+\")), headers=headers)\n",
    "soup = BeautifulSoup(response.text, 'html.parser')\n",
    "\n",
    "# Focus on links within the specified div class\n",
    "result_list_div = soup.find('div', class_='s-main-slot s-result-list s-search-results sg-row')\n",
    "if result_list_div:\n",
    "#     # Extract all relevant links\n",
    "    links = result_list_div.find_all('a', class_=\"a-size-base\", href=True)\n",
    "    links = [link for link in links if link.contents and link.contents[0] in [\"Paperback\", \"Hardcover\"]]\n",
    "\n",
    "    if links:\n",
    "        book_link = \"https://www.amazon.com\" + links[0]['href']\n",
    "\n",
    "        # Follow the first book link (as an example)\n",
    "        detail_response = requests.get(book_link, headers=headers)\n",
    "        detail_soup = BeautifulSoup(detail_response.text, 'html.parser')\n",
    "\n",
    "        # Find and extract book title\n",
    "        title = detail_soup.find(id='productTitle').text.strip()\n",
    "\n",
    "        # Find and extract book author\n",
    "        author_link = detail_soup.find(id='bylineInfo')\n",
    "        author = author_link.find('a').text.strip() if author_link else None\n",
    "\n",
    "        isbn10 = None\n",
    "        isbn13 = None\n",
    "        for item in detail_soup.find_all('li'):\n",
    "            if \"ISBN-10\" in item.text:\n",
    "                isbn10 = re.sub(\"[^0-9-]\", \"\", item.text.split(':')[-1].strip())\n",
    "            elif \"ISBN-13\" in item.text:\n",
    "                isbn13 = re.sub(\"[^0-9-]\", \"\", item.text.split(':')[-1].strip())\n",
    "                \n",
    "        image_link = detail_soup.find('img', id=\"landingImage\")['data-old-hires']\n",
    "\n",
    "        print({\n",
    "            'isbn10': isbn10,\n",
    "            'isbn13': isbn13,\n",
    "            'title': title,\n",
    "            'author': author,\n",
    "            'image_link': image_link\n",
    "        })\n",
    "    else:\n",
    "        print(\"Book link not found.\")\n",
    "else:\n",
    "    print(\"Result list div not found.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "cb30cec1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'isbn10': '0062980149',\n",
       " 'isbn13': '978-0062980144',\n",
       " 'title': 'Operation Pedestal: The Fleet That Battled to Malta, 1942',\n",
       " 'author': 'Max Hastings'}"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import re\n",
    "soup = detail_soup\n",
    "\n",
    "\n",
    "# Find and extract book title\n",
    "title = soup.find(id='productTitle').text.strip()\n",
    "\n",
    "# Find and extract book author\n",
    "author_link = soup.find(id='bylineInfo')\n",
    "author = author_link.find('a').text.strip() if author_link else None\n",
    "\n",
    "# Find and extract high-resolution book image\n",
    "img_div = soup.find(id='imgBlkFront')\n",
    "# high_res_image = img_div.get('data-a-dynamic-image')\n",
    "\n",
    "{\n",
    "    'isbn10': isbn10,\n",
    "    'isbn13': isbn13,\n",
    "    'title': title,\n",
    "    'author': author,\n",
    "    'high_res_image_link': high_res_image_link\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "0d1c524e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "links = result_list_div.find_all('a', class_=\"a-link-normal s-underline-text\", href=True)\n",
    "links"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "1476576f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'User-Agent': 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10.15; rv:124.0) Gecko/20100101 Firefox/124.0', 'Accept-Encoding': 'gzip, deflate, br', 'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/avif,image/webp,*/*;q=0.8', 'Connection': 'keep-alive', 'Accept-Language': 'en-US,en;q=0.5', 'DNT': '1', 'Upgrade-Insecure-Requests': '1', 'Sec-Fetch-Dest': 'document', 'Sec-Fetch-Mode': 'navigate', 'Sec-Fetch-Site': 'same-origin', 'Sec-GPC': '1'}"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response.request.headers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "b6464ed9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import re\n",
    "\n",
    "AMAZON_SEARCH_URL = \"https://www.amazon.com/s?k={}&i=stripbooks\"\n",
    "HEADERS = {\n",
    "    \"User-Agent\": \"Mozilla/5.0 (Macintosh; Intel Mac OS X 10.15; rv:124.0) Gecko/20100101 Firefox/124.0\",\n",
    "    \"Accept\": \"text/html,application/xhtml+xml,application/xml;q=0.9,image/avif,image/webp,*/*;q=0.8\",\n",
    "    \"Accept-Language\": \"en-US,en;q=0.5\",\n",
    "    \"Accept-Encoding\": \"gzip, deflate, br\",\n",
    "    \"DNT\": \"1\",\n",
    "    \"Connection\": \"keep-alive\",\n",
    "    \"Upgrade-Insecure-Requests\": \"1\",\n",
    "    \"Sec-Fetch-Dest\": \"document\",\n",
    "    \"Sec-Fetch-Mode\": \"navigate\",\n",
    "    \"Sec-Fetch-Site\": \"same-origin\",\n",
    "    \"Sec-GPC\": \"1\"\n",
    "}\n",
    "\n",
    "def perform_amazon_search(book_title, author):\n",
    "    \"\"\"Searches Amazon for a book and returns the first relevant link.\"\"\"\n",
    "    query = f\"{book_title} {author}\".replace(\" \", \"+\")\n",
    "    search_response = requests.get(AMAZON_SEARCH_URL.format(query), headers=HEADERS)\n",
    "    search_soup = BeautifulSoup(search_response.text, 'html.parser')\n",
    "\n",
    "    result_div = search_soup.find('div', class_='s-main-slot s-result-list s-search-results sg-row')\n",
    "    return get_first_book_link(result_div)\n",
    "\n",
    "def get_first_book_link(result_div):\n",
    "    \"\"\"Extracts the first relevant book link from Amazon search results.\"\"\"\n",
    "    if not result_div:\n",
    "        return None\n",
    "\n",
    "    links = result_div.find_all('a', class_=\"a-size-base\", href=True)\n",
    "    filtered_links = [link for link in links if link.contents and link.contents[0] in [\"Paperback\", \"Hardcover\"]]\n",
    "\n",
    "    if filtered_links:\n",
    "        return \"https://www.amazon.com\" + filtered_links[0]['href']\n",
    "    else:\n",
    "        return None\n",
    "\n",
    "def extract_book_details(book_url):\n",
    "    \"\"\"Fetches book details from a given Amazon book page.\"\"\"\n",
    "    detail_response = requests.get(book_url, headers=HEADERS)\n",
    "    detail_soup = BeautifulSoup(detail_response.text, 'html.parser')\n",
    "\n",
    "    return {\n",
    "        'isbn10': extract_isbn(detail_soup, \"ISBN-10\"),\n",
    "        'isbn13': extract_isbn(detail_soup, \"ISBN-13\"),\n",
    "        'title': extract_title(detail_soup),\n",
    "        'author': extract_author(detail_soup),\n",
    "        'image_link': extract_image_link(detail_soup)\n",
    "    }\n",
    "\n",
    "def extract_isbn(soup, isbn_type):\n",
    "    \"\"\"Extracts either ISBN-10 or ISBN-13 from the detail page.\"\"\"\n",
    "    isbn = None\n",
    "    for item in soup.find_all('li'):\n",
    "        if isbn_type in item.text:\n",
    "            isbn = re.sub(\"[^0-9-]\", \"\", item.text.split(':')[-1].strip())\n",
    "    return isbn\n",
    "\n",
    "def extract_title(soup):\n",
    "    title_element = soup.find(id='productTitle')\n",
    "    return title_element.text.strip() if title_element else None\n",
    "\n",
    "def extract_author(soup):\n",
    "    author_link = soup.find(id='bylineInfo')\n",
    "    return author_link.find('a').text.strip() if author_link else None\n",
    "\n",
    "def extract_image_link(soup):\n",
    "    image_element = soup.find('img', id=\"landingImage\")\n",
    "    return image_element['data-old-hires'] if image_element else None\n",
    "\n",
    "\n",
    "# book_title = \"Operation Pedestal\"\n",
    "# author = \"Max Hastings\"\n",
    "\n",
    "# book_link = perform_amazon_search(book_title, author)\n",
    "# if book_link:\n",
    "#     book_details = extract_book_details(book_link)\n",
    "#     print(book_details)\n",
    "# else:\n",
    "#     print(\"Book link not found.\")\n",
    "\n",
    "# write a function to clean the book_url by removing all query params\n",
    "def clean_book_link(book_link):\n",
    "    link_without_query = book_link.split('?')[0]\n",
    "    clean_link = re.sub(r'/ref=[^/]+', '', link_without_query)\n",
    "    return clean_link\n",
    "\n",
    "def get_book_from_amazon(book_title, author):\n",
    "    book_link = perform_amazon_search(book_title, author)\n",
    "    book_link = clean_book_link(book_link)\n",
    "    if book_link:\n",
    "        return {'book_url': book_link, **extract_book_details(book_link)}\n",
    "    else:\n",
    "        return None\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "165d13ad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Operation Pedestal by Max Hastings\n",
      "{'book_url': 'https://www.amazon.com/Operation-Pedestal-Fleet-Battled-Malta/dp/0062980149', 'isbn10': '0062980149', 'isbn13': '978-0062980144', 'title': 'Operation Pedestal: The Fleet That Battled to Malta, 1942', 'author': 'Max Hastings', 'image_link': 'https://m.media-amazon.com/images/I/81PCJyPYeuL._SL1500_.jpg'}\n"
     ]
    }
   ],
   "source": [
    "for book in result.books:\n",
    "    print(f\"{book.title} by {book.author}\")\n",
    "    print(get_book_from_amazon(book.title, book.author))\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afe0f0f3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
